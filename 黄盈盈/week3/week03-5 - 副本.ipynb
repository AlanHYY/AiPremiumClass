{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch fashionmnist 数据集 神经网络搭建和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入必要的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms.v2 import ToTensor  #图像的数据转换为张量\n",
    "from torchvision.datasets import KMNIST \n",
    "from torch.utils.data import DataLoader#加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "LR =1e-1\n",
    "epochs = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集 train\n",
    "train_data = KMNIST (root='./fashion_data', train=True, download=True, transform=ToTensor())\n",
    "test_data = KMNIST (root='./fashion_data', train=False, download=True,transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)#加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有结构串联\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 120),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(120, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "#优化器（模型参数更新）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)  #随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.353039503097534\n",
      "Loss: 2.308398723602295\n",
      "Loss: 2.3023223876953125\n",
      "Loss: 2.316194534301758\n",
      "Loss: 2.298456907272339\n",
      "Loss: 2.327479124069214\n",
      "Loss: 2.3070549964904785\n",
      "Loss: 2.308762788772583\n",
      "Loss: 2.304410696029663\n",
      "Loss: 2.2915666103363037\n",
      "Loss: 2.3001415729522705\n",
      "Loss: 2.3086090087890625\n",
      "Loss: 2.314724922180176\n",
      "Loss: 2.3035972118377686\n",
      "Loss: 2.3127779960632324\n",
      "Loss: 2.304276704788208\n",
      "Loss: 2.312391996383667\n",
      "Loss: 2.3098137378692627\n",
      "Loss: 2.304280996322632\n",
      "Loss: 2.3172848224639893\n",
      "Loss: 2.310899257659912\n",
      "Loss: 2.304550886154175\n",
      "Loss: 2.297032356262207\n",
      "Loss: 2.2991950511932373\n",
      "Loss: 2.32358717918396\n",
      "Loss: 2.315683364868164\n",
      "Loss: 2.299241781234741\n",
      "Loss: 2.3229033946990967\n",
      "Loss: 2.303657054901123\n",
      "Loss: 2.312373399734497\n",
      "Loss: 2.305619955062866\n",
      "Loss: 2.307042360305786\n",
      "Loss: 2.3013107776641846\n",
      "Loss: 2.3092081546783447\n",
      "Loss: 2.2981646060943604\n",
      "Loss: 2.3187336921691895\n",
      "Loss: 2.290661573410034\n",
      "Loss: 2.29764986038208\n",
      "Loss: 2.326655387878418\n",
      "Loss: 2.3140761852264404\n",
      "Loss: 2.2987606525421143\n",
      "Loss: 2.3138113021850586\n",
      "Loss: 2.305513620376587\n",
      "Loss: 2.3087918758392334\n",
      "Loss: 2.2998459339141846\n",
      "Loss: 2.3190243244171143\n",
      "Loss: 2.308385133743286\n",
      "Loss: 2.3019301891326904\n",
      "Loss: 2.300924777984619\n",
      "Loss: 2.308741331100464\n",
      "Loss: 2.298682451248169\n",
      "Loss: 2.3062431812286377\n",
      "Loss: 2.3001105785369873\n",
      "Loss: 2.2930593490600586\n",
      "Loss: 2.3321993350982666\n",
      "Loss: 2.2989821434020996\n",
      "Loss: 2.3095033168792725\n",
      "Loss: 2.3028011322021484\n",
      "Loss: 2.300114154815674\n",
      "Loss: 2.2973690032958984\n",
      "Loss: 2.3202555179595947\n",
      "Loss: 2.3027868270874023\n",
      "Loss: 2.305579423904419\n",
      "Loss: 2.307943105697632\n",
      "Loss: 2.293776750564575\n",
      "Loss: 2.3104960918426514\n",
      "Loss: 2.2988007068634033\n",
      "Loss: 2.310020923614502\n",
      "Loss: 2.3105194568634033\n",
      "Loss: 2.307459831237793\n",
      "Loss: 2.297330141067505\n",
      "Loss: 2.3063762187957764\n",
      "Loss: 2.301856279373169\n",
      "Loss: 2.304920196533203\n",
      "Loss: 2.303740978240967\n",
      "Loss: 2.2974400520324707\n",
      "Loss: 2.309227705001831\n",
      "Loss: 2.3013298511505127\n",
      "Loss: 2.3016457557678223\n",
      "Loss: 2.298527717590332\n",
      "Loss: 2.304358959197998\n",
      "Loss: 2.3152387142181396\n",
      "Loss: 2.2953391075134277\n",
      "Loss: 2.310568332672119\n",
      "Loss: 2.2981135845184326\n",
      "Loss: 2.2984964847564697\n",
      "Loss: 2.3046069145202637\n",
      "Loss: 2.28922963142395\n",
      "Loss: 2.332491159439087\n",
      "Loss: 2.2967147827148438\n",
      "Loss: 2.315627336502075\n",
      "Loss: 2.2945408821105957\n",
      "Loss: 2.2916548252105713\n",
      "Loss: 2.310223340988159\n",
      "Loss: 2.3059587478637695\n",
      "Loss: 2.2937512397766113\n",
      "Loss: 2.3094613552093506\n",
      "Loss: 2.2991247177124023\n",
      "Loss: 2.295680284500122\n",
      "Loss: 2.311798334121704\n",
      "Loss: 2.2978429794311523\n",
      "Loss: 2.3161044120788574\n",
      "Loss: 2.30983829498291\n",
      "Loss: 2.302783966064453\n",
      "Loss: 2.2923367023468018\n",
      "Loss: 2.303644895553589\n",
      "Loss: 2.31475567817688\n",
      "Loss: 2.3138234615325928\n",
      "Loss: 2.3027687072753906\n",
      "Loss: 2.2881405353546143\n",
      "Loss: 2.29612135887146\n",
      "Loss: 2.2888736724853516\n",
      "Loss: 2.299269199371338\n",
      "Loss: 2.289121150970459\n",
      "Loss: 2.3265318870544434\n",
      "Loss: 2.2913925647735596\n",
      "Loss: 2.286360502243042\n",
      "Loss: 2.307279586791992\n",
      "Loss: 2.3177108764648438\n",
      "Loss: 2.291821241378784\n",
      "Loss: 2.3117191791534424\n",
      "Loss: 2.303424596786499\n",
      "Loss: 2.298826217651367\n",
      "Loss: 2.27632737159729\n",
      "Loss: 2.2861530780792236\n",
      "Loss: 2.3097190856933594\n",
      "Loss: 2.315631866455078\n",
      "Loss: 2.307224988937378\n",
      "Loss: 2.278348445892334\n",
      "Loss: 2.3004846572875977\n",
      "Loss: 2.308532238006592\n",
      "Loss: 2.2993264198303223\n",
      "Loss: 2.314701557159424\n",
      "Loss: 2.2929718494415283\n",
      "Loss: 2.285313129425049\n",
      "Loss: 2.2975261211395264\n",
      "Loss: 2.306049346923828\n",
      "Loss: 2.2771010398864746\n",
      "Loss: 2.306701183319092\n",
      "Loss: 2.288936138153076\n",
      "Loss: 2.280151128768921\n",
      "Loss: 2.2989282608032227\n",
      "Loss: 2.3126134872436523\n",
      "Loss: 2.289954423904419\n",
      "Loss: 2.293175220489502\n",
      "Loss: 2.3042445182800293\n",
      "Loss: 2.2910313606262207\n",
      "Loss: 2.2926571369171143\n",
      "Loss: 2.293844223022461\n",
      "Loss: 2.2990267276763916\n",
      "Loss: 2.3210394382476807\n",
      "Loss: 2.293264865875244\n",
      "Loss: 2.293646812438965\n",
      "Loss: 2.2941970825195312\n",
      "Loss: 2.2940258979797363\n",
      "Loss: 2.3058221340179443\n",
      "Loss: 2.3098533153533936\n",
      "Loss: 2.3029613494873047\n",
      "Loss: 2.292743444442749\n",
      "Loss: 2.2977261543273926\n",
      "Loss: 2.3061633110046387\n",
      "Loss: 2.2888362407684326\n",
      "Loss: 2.2946877479553223\n",
      "Loss: 2.286888360977173\n",
      "Loss: 2.2777340412139893\n",
      "Loss: 2.3245315551757812\n",
      "Loss: 2.3085978031158447\n",
      "Loss: 2.315873146057129\n",
      "Loss: 2.293124198913574\n",
      "Loss: 2.28495454788208\n",
      "Loss: 2.3023364543914795\n",
      "Loss: 2.295498847961426\n",
      "Loss: 2.3021535873413086\n",
      "Loss: 2.2942352294921875\n",
      "Loss: 2.287471294403076\n",
      "Loss: 2.2925703525543213\n",
      "Loss: 2.306058168411255\n",
      "Loss: 2.2877726554870605\n",
      "Loss: 2.310565233230591\n",
      "Loss: 2.3067450523376465\n",
      "Loss: 2.3041741847991943\n",
      "Loss: 2.3011796474456787\n",
      "Loss: 2.2911601066589355\n",
      "Loss: 2.292482614517212\n",
      "Loss: 2.3005473613739014\n",
      "Loss: 2.302361488342285\n",
      "Loss: 2.2867212295532227\n",
      "Loss: 2.2818565368652344\n",
      "Loss: 2.307677984237671\n",
      "Loss: 2.2806904315948486\n",
      "Loss: 2.304952621459961\n",
      "Loss: 2.278012990951538\n",
      "Loss: 2.2976479530334473\n",
      "Loss: 2.2987987995147705\n",
      "Loss: 2.2837870121002197\n",
      "Loss: 2.268512725830078\n",
      "Loss: 2.295758008956909\n",
      "Loss: 2.286867141723633\n",
      "Loss: 2.313032388687134\n",
      "Loss: 2.2964041233062744\n",
      "Loss: 2.285003662109375\n",
      "Loss: 2.3049981594085693\n",
      "Loss: 2.268691062927246\n",
      "Loss: 2.3015761375427246\n",
      "Loss: 2.2977232933044434\n",
      "Loss: 2.314423084259033\n",
      "Loss: 2.303826332092285\n",
      "Loss: 2.2908272743225098\n",
      "Loss: 2.2887959480285645\n",
      "Loss: 2.2914421558380127\n",
      "Loss: 2.287416458129883\n",
      "Loss: 2.290757179260254\n",
      "Loss: 2.2845072746276855\n",
      "Loss: 2.3038668632507324\n",
      "Loss: 2.2966110706329346\n",
      "Loss: 2.282275438308716\n",
      "Loss: 2.2954907417297363\n",
      "Loss: 2.2788784503936768\n",
      "Loss: 2.279191732406616\n",
      "Loss: 2.311664581298828\n",
      "Loss: 2.284679651260376\n",
      "Loss: 2.3157706260681152\n",
      "Loss: 2.293187379837036\n",
      "Loss: 2.2943227291107178\n",
      "Loss: 2.2865724563598633\n",
      "Loss: 2.28747296333313\n",
      "Loss: 2.295947551727295\n",
      "Loss: 2.281606435775757\n",
      "Loss: 2.290611743927002\n",
      "Loss: 2.2761003971099854\n",
      "Loss: 2.2908613681793213\n",
      "Loss: 2.3097712993621826\n",
      "Loss: 2.282414674758911\n",
      "Loss: 2.2833056449890137\n",
      "Loss: 2.283790349960327\n",
      "Loss: 2.2946090698242188\n",
      "Loss: 2.2830116748809814\n",
      "Loss: 2.3009233474731445\n",
      "Loss: 2.285428524017334\n",
      "Loss: 2.290571689605713\n",
      "Loss: 2.2692391872406006\n",
      "Loss: 2.292550802230835\n",
      "Loss: 2.2719905376434326\n",
      "Loss: 2.3025872707366943\n",
      "Loss: 2.3062682151794434\n",
      "Loss: 2.2772769927978516\n",
      "Loss: 2.299926519393921\n",
      "Loss: 2.2931301593780518\n",
      "Loss: 2.30718994140625\n",
      "Loss: 2.278628349304199\n",
      "Loss: 2.29982852935791\n",
      "Loss: 2.2810416221618652\n",
      "Loss: 2.278918981552124\n",
      "Loss: 2.289295196533203\n",
      "Loss: 2.287066698074341\n",
      "Loss: 2.2815639972686768\n",
      "Loss: 2.283337354660034\n",
      "Loss: 2.289757490158081\n",
      "Loss: 2.2855138778686523\n",
      "Loss: 2.2757527828216553\n",
      "Loss: 2.280313014984131\n",
      "Loss: 2.280858039855957\n",
      "Loss: 2.2854838371276855\n",
      "Loss: 2.289463996887207\n",
      "Loss: 2.2786684036254883\n",
      "Loss: 2.272963762283325\n",
      "Loss: 2.2963099479675293\n",
      "Loss: 2.2869861125946045\n",
      "Loss: 2.280007839202881\n",
      "Loss: 2.2973074913024902\n",
      "Loss: 2.2769997119903564\n",
      "Loss: 2.2760214805603027\n",
      "Loss: 2.2946372032165527\n",
      "Loss: 2.282534599304199\n",
      "Loss: 2.2898473739624023\n",
      "Loss: 2.2814345359802246\n",
      "Loss: 2.286226272583008\n",
      "Loss: 2.2916250228881836\n",
      "Loss: 2.286081314086914\n",
      "Loss: 2.2615952491760254\n",
      "Loss: 2.295620918273926\n",
      "Loss: 2.2657084465026855\n",
      "Loss: 2.294692039489746\n",
      "Loss: 2.2672338485717773\n",
      "Loss: 2.298569679260254\n",
      "Loss: 2.2820019721984863\n",
      "Loss: 2.2870442867279053\n",
      "Loss: 2.268009901046753\n",
      "Loss: 2.265624523162842\n",
      "Loss: 2.29992413520813\n",
      "Loss: 2.2727675437927246\n",
      "Loss: 2.2759764194488525\n",
      "Loss: 2.281635284423828\n",
      "Loss: 2.2863545417785645\n",
      "Loss: 2.257412910461426\n",
      "Loss: 2.278747797012329\n",
      "Loss: 2.2851827144622803\n",
      "Loss: 2.2751681804656982\n",
      "Loss: 2.277573585510254\n",
      "Loss: 2.2727975845336914\n",
      "Loss: 2.284555673599243\n",
      "Loss: 2.285693645477295\n",
      "Loss: 2.2690558433532715\n",
      "Loss: 2.27266788482666\n",
      "Loss: 2.27316951751709\n",
      "Loss: 2.307614803314209\n",
      "Loss: 2.271688938140869\n",
      "Loss: 2.263875961303711\n",
      "Loss: 2.2759323120117188\n",
      "Loss: 2.278949022293091\n",
      "Loss: 2.2743139266967773\n",
      "Loss: 2.260874032974243\n",
      "Loss: 2.2732863426208496\n",
      "Loss: 2.2614150047302246\n",
      "Loss: 2.269805908203125\n",
      "Loss: 2.2776501178741455\n",
      "Loss: 2.282406806945801\n",
      "Loss: 2.277592658996582\n",
      "Loss: 2.270179510116577\n",
      "Loss: 2.2781906127929688\n",
      "Loss: 2.2810277938842773\n",
      "Loss: 2.2778170108795166\n",
      "Loss: 2.2604737281799316\n",
      "Loss: 2.2705442905426025\n",
      "Loss: 2.2679812908172607\n",
      "Loss: 2.2633280754089355\n",
      "Loss: 2.2807748317718506\n",
      "Loss: 2.2608044147491455\n",
      "Loss: 2.280550479888916\n",
      "Loss: 2.278458833694458\n",
      "Loss: 2.2562036514282227\n",
      "Loss: 2.284031629562378\n",
      "Loss: 2.2781906127929688\n",
      "Loss: 2.268282890319824\n",
      "Loss: 2.2648332118988037\n",
      "Loss: 2.270106554031372\n",
      "Loss: 2.2476234436035156\n",
      "Loss: 2.2826461791992188\n",
      "Loss: 2.269169330596924\n",
      "Loss: 2.2668492794036865\n",
      "Loss: 2.263789653778076\n",
      "Loss: 2.27245831489563\n",
      "Loss: 2.2504427433013916\n",
      "Loss: 2.283769369125366\n",
      "Loss: 2.2483291625976562\n",
      "Loss: 2.2575502395629883\n",
      "Loss: 2.274416208267212\n",
      "Loss: 2.267723560333252\n",
      "Loss: 2.27832293510437\n",
      "Loss: 2.2587532997131348\n",
      "Loss: 2.25216007232666\n",
      "Loss: 2.2687697410583496\n",
      "Loss: 2.2645514011383057\n",
      "Loss: 2.2664413452148438\n",
      "Loss: 2.249189853668213\n",
      "Loss: 2.2768094539642334\n",
      "Loss: 2.254486560821533\n",
      "Loss: 2.259982109069824\n",
      "Loss: 2.2571845054626465\n",
      "Loss: 2.2551498413085938\n",
      "Loss: 2.2457334995269775\n",
      "Loss: 2.2695202827453613\n",
      "Loss: 2.2679598331451416\n",
      "Loss: 2.251917600631714\n",
      "Loss: 2.234611988067627\n",
      "Loss: 2.2652084827423096\n",
      "Loss: 2.2632198333740234\n",
      "Loss: 2.2492411136627197\n",
      "Loss: 2.259490728378296\n",
      "Loss: 2.2397141456604004\n",
      "Loss: 2.2451090812683105\n",
      "Loss: 2.22938871383667\n",
      "Loss: 2.2611749172210693\n",
      "Loss: 2.282423496246338\n",
      "Loss: 2.247075080871582\n",
      "Loss: 2.2463021278381348\n",
      "Loss: 2.255831003189087\n",
      "Loss: 2.2443954944610596\n",
      "Loss: 2.250244140625\n",
      "Loss: 2.2389280796051025\n",
      "Loss: 2.259983777999878\n",
      "Loss: 2.2327804565429688\n",
      "Loss: 2.2631523609161377\n",
      "Loss: 2.2334542274475098\n",
      "Loss: 2.250260829925537\n",
      "Loss: 2.245863199234009\n",
      "Loss: 2.232583999633789\n",
      "Loss: 2.2460861206054688\n",
      "Loss: 2.233879327774048\n",
      "Loss: 2.2200045585632324\n",
      "Loss: 2.262108325958252\n",
      "Loss: 2.2293007373809814\n",
      "Loss: 2.2321970462799072\n",
      "Loss: 2.2504639625549316\n",
      "Loss: 2.223052978515625\n",
      "Loss: 2.248349666595459\n",
      "Loss: 2.2184677124023438\n",
      "Loss: 2.202820301055908\n",
      "Loss: 2.290022611618042\n",
      "Loss: 2.218456745147705\n",
      "Loss: 2.233245372772217\n",
      "Loss: 2.2209665775299072\n",
      "Loss: 2.2332394123077393\n",
      "Loss: 2.259974479675293\n",
      "Loss: 2.2283437252044678\n",
      "Loss: 2.2077765464782715\n",
      "Loss: 2.2396161556243896\n",
      "Loss: 2.230536699295044\n",
      "Loss: 2.217353105545044\n",
      "Loss: 2.2030975818634033\n",
      "Loss: 2.225351333618164\n",
      "Loss: 2.237851858139038\n",
      "Loss: 2.215973138809204\n",
      "Loss: 2.2276294231414795\n",
      "Loss: 2.2315831184387207\n",
      "Loss: 2.2235238552093506\n",
      "Loss: 2.2084524631500244\n",
      "Loss: 2.1976490020751953\n",
      "Loss: 2.1986329555511475\n",
      "Loss: 2.196221113204956\n",
      "Loss: 2.2110679149627686\n",
      "Loss: 2.2088701725006104\n",
      "Loss: 2.211778163909912\n",
      "Loss: 2.2179272174835205\n",
      "Loss: 2.205829381942749\n",
      "Loss: 2.230790376663208\n",
      "Loss: 2.221618413925171\n",
      "Loss: 2.205597162246704\n",
      "Loss: 2.2027270793914795\n",
      "Loss: 2.2029106616973877\n",
      "Loss: 2.1997454166412354\n",
      "Loss: 2.193319082260132\n",
      "Loss: 2.2195544242858887\n",
      "Loss: 2.216651201248169\n",
      "Loss: 2.2229528427124023\n",
      "Loss: 2.1951904296875\n",
      "Loss: 2.1925852298736572\n",
      "Loss: 2.2024319171905518\n",
      "Loss: 2.1751062870025635\n",
      "Loss: 2.205146312713623\n",
      "Loss: 2.174891948699951\n",
      "Loss: 2.2091546058654785\n",
      "Loss: 2.190821647644043\n",
      "Loss: 2.193331718444824\n",
      "Loss: 2.1986610889434814\n",
      "Loss: 2.194695234298706\n",
      "Loss: 2.1750168800354004\n",
      "Loss: 2.2113051414489746\n",
      "Loss: 2.2080328464508057\n",
      "Loss: 2.173424482345581\n",
      "Loss: 2.1911628246307373\n",
      "Loss: 2.1884779930114746\n",
      "Loss: 2.147326946258545\n",
      "Loss: 2.155660629272461\n",
      "Loss: 2.1493239402770996\n",
      "Loss: 2.177523612976074\n",
      "Loss: 2.2078144550323486\n",
      "Loss: 2.180079460144043\n",
      "Loss: 2.182384490966797\n",
      "Loss: 2.151374101638794\n",
      "Loss: 2.18064022064209\n",
      "Loss: 2.171470880508423\n",
      "Loss: 2.1572296619415283\n",
      "Loss: 2.165661334991455\n",
      "Loss: 2.1809797286987305\n",
      "Loss: 2.142573118209839\n",
      "Loss: 2.2021358013153076\n",
      "Loss: 2.168799877166748\n",
      "Loss: 2.1364848613739014\n"
     ]
    }
   ],
   "source": [
    "#for epoch in epochs:\n",
    "    #提取训练数据\n",
    "for data, target in trian_dl:\n",
    "        #前向运算\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        #计算损失\n",
    "        loss = loss_fn(output, target)\n",
    "        #反向传播\n",
    "        optimizer.zero_grad()#梯度清零\n",
    "        loss.backward() #计算梯度，保存参数\n",
    "        optimizer.step() #更新参数\n",
    "\n",
    "        print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 23.3%\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 不计算梯度\n",
    "    for data, target in test_dl:\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        _, predicted = torch.max(output, 1)  # 返回每行最大值和索引\n",
    "        total += target.size(0)  # size(0) 等效 shape[0]\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct/total*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
