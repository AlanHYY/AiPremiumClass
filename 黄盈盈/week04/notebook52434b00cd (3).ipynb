{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### pytorch数据集 神经网络搭建和训练","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n# 超参数设置\nLR = 1e-3\nEPOCHS = 20\nBATCH_SIZE = 32\n\n# 数据预处理与加载\n# ------------------------------------------------\n# 加载Olivetti Faces数据集\nolivetti = fetch_olivetti_faces()\ndata = olivetti.data.astype(np.float32)  # 转换为float32类型\ntargets = olivetti.target\n\n# 按类别划分训练集（每个类前7个样本）和测试集（每个类后3个样本）\ntrain_indices, test_indices = [], []\nfor i in range(40):\n    train_indices.extend(range(i*10, i*10+7))\n    test_indices.extend(range(i*10+7, (i+1)*10))\n\n\nX_train, y_train = data[train_indices], targets[train_indices]\nX_test, y_test = data[test_indices], targets[test_indices]\n\n# 数据标准化（归一化）\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0) + 1e-8  # 防止除零\nX_train = (X_train - mean) / std\nX_test = (X_test - mean) / std\n\n# 自定义Dataset类\nclass OlivettiDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# 创建DataLoader\ntrain_dataset = OlivettiDataset(X_train, y_train)\ntest_dataset = OlivettiDataset(X_test, y_test)\ntrain_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n\n# 模型定义（包含归一化和正则化）\n# ------------------------------------------------\nclass TorchNN(nn.Module):\n    def __init__(self, input_size=4096, hidden_size=512, output_size=40):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.BatchNorm1d(hidden_size),      # 批归一化\n            nn.ReLU(),\n            nn.Dropout(0.5),                  # Dropout正则化\n            nn.Linear(hidden_size, hidden_size//2),\n            nn.BatchNorm1d(hidden_size//2),   # 批归一化\n            nn.ReLU(),\n            nn.Dropout(0.3),                 # Dropout正则化\n            nn.Linear(hidden_size//2, output_size)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\n# 训练与测试函数\n# ------------------------------------------------\ndef train_model(model, optimizer, loss_fn, train_loader, epochs):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = loss_fn(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * X_batch.size(0)\n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n\ndef test_model(model, test_loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n    print(f'Test Accuracy: {100 * correct/total:.2f}%')\n\n# 实验：不同优化器对比\n# ------------------------------------------------\noptimizers = {\n    'Adam': optim.Adam,\n    'SGD': optim.SGD,\n    'RMSprop': optim.RMSprop\n}\n\n# 配置不同优化器参数\noptimizer_configs = {\n    'Adam': {'lr': LR, 'weight_decay': 1e-4},          # L2正则化\n    'SGD': {'lr': LR, 'momentum': 0.9, 'weight_decay': 1e-4},\n    'RMSprop': {'lr': LR, 'weight_decay': 1e-4}\n}\n\n# 遍历所有优化器进行实验\nfor opt_name in optimizers:\n    print(f\"\\n=== 当前优化器：{opt_name} ===\")\n    model = TorchNN()\n    loss_fn = nn.CrossEntropyLoss()\n    \n    # 初始化优化器\n    if opt_name == 'SGD':\n        optimizer = optimizers[opt_name](model.parameters(), **optimizer_configs[opt_name])\n    else:\n        optimizer = optimizers[opt_name](model.parameters(), lr=LR, weight_decay=1e-4)\n    \n    # 训练与验证\n    train_model(model, optimizer, loss_fn, train_dl, EPOCHS)\n    test_model(model, test_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T12:52:38.740647Z","iopub.execute_input":"2025-03-20T12:52:38.740939Z","iopub.status.idle":"2025-03-20T12:52:48.661422Z","shell.execute_reply.started":"2025-03-20T12:52:38.740917Z","shell.execute_reply":"2025-03-20T12:52:48.660565Z"}},"outputs":[{"name":"stdout","text":"\n=== 当前优化器：Adam ===\nEpoch [1/20], Loss: 3.4103\nEpoch [2/20], Loss: 2.6345\nEpoch [3/20], Loss: 2.1163\nEpoch [4/20], Loss: 1.7481\nEpoch [5/20], Loss: 1.3962\nEpoch [6/20], Loss: 1.1046\nEpoch [7/20], Loss: 0.8600\nEpoch [8/20], Loss: 0.7320\nEpoch [9/20], Loss: 0.5477\nEpoch [10/20], Loss: 0.4458\nEpoch [11/20], Loss: 0.3364\nEpoch [12/20], Loss: 0.2743\nEpoch [13/20], Loss: 0.2304\nEpoch [14/20], Loss: 0.1984\nEpoch [15/20], Loss: 0.1686\nEpoch [16/20], Loss: 0.1401\nEpoch [17/20], Loss: 0.1250\nEpoch [18/20], Loss: 0.1176\nEpoch [19/20], Loss: 0.1027\nEpoch [20/20], Loss: 0.0780\nTest Accuracy: 97.50%\n\n=== 当前优化器：SGD ===\nEpoch [1/20], Loss: 3.7121\nEpoch [2/20], Loss: 3.3841\nEpoch [3/20], Loss: 3.1049\nEpoch [4/20], Loss: 2.8290\nEpoch [5/20], Loss: 2.6075\nEpoch [6/20], Loss: 2.4211\nEpoch [7/20], Loss: 2.2787\nEpoch [8/20], Loss: 2.1859\nEpoch [9/20], Loss: 2.0270\nEpoch [10/20], Loss: 1.9377\nEpoch [11/20], Loss: 1.8155\nEpoch [12/20], Loss: 1.7443\nEpoch [13/20], Loss: 1.6388\nEpoch [14/20], Loss: 1.5394\nEpoch [15/20], Loss: 1.4477\nEpoch [16/20], Loss: 1.3721\nEpoch [17/20], Loss: 1.3474\nEpoch [18/20], Loss: 1.2376\nEpoch [19/20], Loss: 1.1904\nEpoch [20/20], Loss: 1.1815\nTest Accuracy: 94.17%\n\n=== 当前优化器：RMSprop ===\nEpoch [1/20], Loss: 3.3712\nEpoch [2/20], Loss: 2.0310\nEpoch [3/20], Loss: 1.3365\nEpoch [4/20], Loss: 0.9283\nEpoch [5/20], Loss: 0.6907\nEpoch [6/20], Loss: 0.5070\nEpoch [7/20], Loss: 0.3615\nEpoch [8/20], Loss: 0.2997\nEpoch [9/20], Loss: 0.1942\nEpoch [10/20], Loss: 0.1749\nEpoch [11/20], Loss: 0.1657\nEpoch [12/20], Loss: 0.1477\nEpoch [13/20], Loss: 0.1082\nEpoch [14/20], Loss: 0.1006\nEpoch [15/20], Loss: 0.0907\nEpoch [16/20], Loss: 0.0858\nEpoch [17/20], Loss: 0.0847\nEpoch [18/20], Loss: 0.0626\nEpoch [19/20], Loss: 0.0696\nEpoch [20/20], Loss: 0.0798\nTest Accuracy: 98.33%\n","output_type":"stream"}],"execution_count":11}]}